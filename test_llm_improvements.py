"""
Test script for LLM improvements including streaming and configuration.

This script tests the new streaming generation, advanced configuration,
and enhanced thinking pad features.
"""

import asyncio
import time
import logging
from typing import Dict, Any
import duckdb

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import our new modules
from src.duckdb_analytics.llm.streaming_generator import (
    StreamingSQLGenerator,
    StreamChunk,
    StreamType,
    ThinkingStages
)
from src.duckdb_analytics.llm.model_config import (
    ModelConfigManager,
    ModelProfile,
    GenerationMode,
    AdaptiveModelSelector
)
from src.duckdb_analytics.llm.integrated_generator import (
    IntegratedSQLGenerator,
    GenerationResult
)


def test_configuration_system():
    """Test the advanced configuration system."""
    print("\n" + "="*60)
    print("Testing Configuration System")
    print("="*60)
    
    # Create config manager
    config_manager = ModelConfigManager()
    
    # List available profiles
    profiles = config_manager.list_profiles()
    print(f"\nAvailable profiles: {profiles}")
    
    # Create test profiles for different modes
    test_profiles = [
        ("test_fast", GenerationMode.FAST),
        ("test_balanced", GenerationMode.BALANCED),
        ("test_thorough", GenerationMode.THOROUGH)
    ]
    
    for name, mode in test_profiles:
        profile = config_manager.create_profile(
            name=name,
            provider="lm_studio",
            model_id="meta-llama-3.1-8b-instruct",
            mode=mode
        )
        print(f"\nCreated profile '{name}':")
        print(f"  - Temperature: {profile.temperature}")
        print(f"  - Max tokens: {profile.max_tokens}")
        print(f"  - Thinking depth: {profile.thinking_depth}")
        print(f"  - Timeout: {profile.request_timeout}s")
    
    # Test optimization
    print("\nOptimizing 'test_balanced' for latency...")
    config_manager.optimize_for_latency("test_balanced")
    optimized = config_manager.get_profile("test_balanced")
    print(f"  - New temperature: {optimized.temperature}")
    print(f"  - New max tokens: {optimized.max_tokens}")
    print(f"  - New thinking depth: {optimized.thinking_depth}")
    
    return config_manager


async def test_streaming_generation():
    """Test the streaming SQL generation."""
    print("\n" + "="*60)
    print("Testing Streaming Generation")
    print("="*60)
    
    # Create streaming generator
    generator = StreamingSQLGenerator(
        base_url="http://localhost:1234/v1",
        model="meta-llama-3.1-8b-instruct",
        profile="balanced"
    )
    
    # Test query
    query = "Show me total sales by month for the last year"
    
    # Mock schema context
    schema = {
        "sales": {
            "columns": [
                {"name": "id", "type": "INTEGER"},
                {"name": "date", "type": "DATE"},
                {"name": "amount", "type": "DECIMAL"},
                {"name": "customer_id", "type": "INTEGER"}
            ]
        }
    }
    
    print(f"\nQuery: {query}")
    print("\nStreaming generation:")
    print("-" * 40)
    
    thinking_chunks = []
    sql_chunks = []
    
    try:
        async for chunk in generator.generate_with_streaming(
            natural_language_query=query,
            schema_context=schema,
            system_prompt="Generate DuckDB SQL for the given query."
        ):
            if chunk.type == StreamType.THINKING:
                print(f"üí≠ {chunk.content}")
                thinking_chunks.append(chunk)
                await asyncio.sleep(0.1)  # Simulate UI update
                
            elif chunk.type == StreamType.SQL:
                print(f"üìù SQL: {chunk.content}", end="")
                sql_chunks.append(chunk)
                
            elif chunk.type == StreamType.COMPLETE:
                print(f"\n‚úÖ Generation complete!")
                print(f"   Metadata: {chunk.metadata}")
                
            elif chunk.type == StreamType.ERROR:
                print(f"\n‚ùå Error: {chunk.content}")
    
    except Exception as e:
        print(f"\n‚ùå Streaming failed: {str(e)}")
        print("Note: This test requires LM Studio to be running on localhost:1234")
    
    return thinking_chunks, sql_chunks


def test_integrated_generator():
    """Test the integrated SQL generator."""
    print("\n" + "="*60)
    print("Testing Integrated Generator")
    print("="*60)
    
    # Create a mock DuckDB connection
    conn = duckdb.connect(":memory:")
    
    # Create test table
    conn.execute("""
        CREATE TABLE sales (
            id INTEGER,
            date DATE,
            amount DECIMAL(10,2),
            customer_id INTEGER
        )
    """)
    
    # Insert sample data
    conn.execute("""
        INSERT INTO sales VALUES
        (1, '2024-01-15', 100.50, 1),
        (2, '2024-01-20', 250.00, 2),
        (3, '2024-02-10', 175.25, 1)
    """)
    
    # Create integrated generator
    generator = IntegratedSQLGenerator(
        duckdb_conn=conn,
        base_url="http://localhost:1234/v1",
        enable_streaming=False,  # Disable streaming for simpler test
        enable_adaptive=True
    )
    
    # Test queries with different complexities
    test_queries = [
        ("Show all sales", GenerationMode.FAST),
        ("Calculate total sales by month", GenerationMode.BALANCED),
        ("Find the top customers by total purchase amount with their transaction count", GenerationMode.THOROUGH)
    ]
    
    for query, mode in test_queries:
        print(f"\n{'='*40}")
        print(f"Query: {query}")
        print(f"Mode: {mode.value}")
        print("-" * 40)
        
        start_time = time.time()
        
        try:
            result = generator.generate_sql(
                natural_language_query=query,
                mode=mode
            )
            
            generation_time = time.time() - start_time
            
            if result.sql:
                print(f"‚úÖ SQL Generated:")
                print(f"   {result.sql[:100]}..." if len(result.sql) > 100 else f"   {result.sql}")
                print(f"‚è±Ô∏è  Generation time: {generation_time:.2f}s")
                print(f"üìä Confidence: {result.confidence:.0%}")
                print(f"üéØ Profile used: {result.profile_used}")
                
                # Try to execute the SQL
                try:
                    conn.execute(result.sql)
                    print(f"‚úÖ SQL executed successfully")
                except Exception as e:
                    print(f"‚ö†Ô∏è  SQL execution failed: {str(e)}")
            else:
                print(f"‚ùå Generation failed: {result.error}")
                
        except Exception as e:
            print(f"‚ùå Test failed: {str(e)}")
            print("Note: This test requires LM Studio to be running")
    
    # Show metrics
    print("\n" + "="*40)
    print("Generation Metrics:")
    metrics = generator.get_metrics()
    for key, value in metrics.items():
        print(f"  - {key}: {value}")
    
    conn.close()


def test_latency_improvements():
    """Test and measure latency improvements."""
    print("\n" + "="*60)
    print("Testing Latency Improvements")
    print("="*60)
    
    # Create mock connection
    conn = duckdb.connect(":memory:")
    
    # Test configurations
    configs = [
        ("Original", {"enable_streaming": False, "enable_adaptive": False}),
        ("With Adaptive", {"enable_streaming": False, "enable_adaptive": True}),
        ("With Streaming", {"enable_streaming": True, "enable_adaptive": False}),
        ("Full Optimization", {"enable_streaming": True, "enable_adaptive": True})
    ]
    
    query = "Show me the top 5 products by revenue"
    
    print(f"\nTest Query: {query}")
    print("\nLatency Comparison:")
    print("-" * 40)
    
    results = []
    
    for name, config in configs:
        try:
            generator = IntegratedSQLGenerator(
                duckdb_conn=conn,
                base_url="http://localhost:1234/v1",
                **config
            )
            
            # Measure generation time
            start = time.time()
            result = generator.generate_sql(query)
            elapsed = time.time() - start
            
            results.append((name, elapsed, result.sql is not None))
            print(f"{name:20} : {elapsed:.3f}s {'‚úÖ' if result.sql else '‚ùå'}")
            
        except Exception as e:
            results.append((name, 0, False))
            print(f"{name:20} : Failed - {str(e)[:30]}...")
    
    # Calculate improvements
    if results[0][1] > 0:  # If original succeeded
        print("\nImprovement Analysis:")
        print("-" * 40)
        baseline = results[0][1]
        for name, elapsed, success in results[1:]:
            if success and elapsed > 0:
                improvement = ((baseline - elapsed) / baseline) * 100
                print(f"{name:20} : {improvement:+.1f}% {'faster' if improvement > 0 else 'slower'}")
    
    conn.close()


def main():
    """Run all tests."""
    print("\n" + "="*60)
    print("LLM Improvements Test Suite")
    print("="*60)
    
    # Note about requirements
    print("\n‚ö†Ô∏è  Note: These tests require:")
    print("   1. LM Studio running on localhost:1234")
    print("   2. A model loaded in LM Studio")
    print("   3. The model to be compatible with OpenAI API format")
    
    # Run tests
    try:
        # Test 1: Configuration System
        config_manager = test_configuration_system()
        print("‚úÖ Configuration system test complete")
        
        # Test 2: Streaming Generation
        print("\nüîÑ Testing streaming generation...")
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        thinking, sql = loop.run_until_complete(test_streaming_generation())
        print(f"‚úÖ Streaming test complete: {len(thinking)} thinking chunks, {len(sql)} SQL chunks")
        
        # Test 3: Integrated Generator
        test_integrated_generator()
        print("‚úÖ Integrated generator test complete")
        
        # Test 4: Latency Improvements
        test_latency_improvements()
        print("‚úÖ Latency improvement test complete")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Tests interrupted by user")
    except Exception as e:
        print(f"\n\n‚ùå Test suite failed: {str(e)}")
    
    print("\n" + "="*60)
    print("Test Suite Complete")
    print("="*60)
    
    # Summary of improvements
    print("\nüìä Summary of Improvements:")
    print("-" * 40)
    print("1. ‚úÖ Streaming SQL generation with live thinking pad")
    print("2. ‚úÖ Advanced configuration system with profiles")
    print("3. ‚úÖ Adaptive model selection based on query complexity")
    print("4. ‚úÖ Optimized latency with parallel processing")
    print("5. ‚úÖ Enhanced UI components for better user experience")
    print("6. ‚úÖ Multiple generation modes (Fast/Balanced/Thorough)")
    print("7. ‚úÖ Confidence scoring and progress visualization")
    
    print("\nüéØ Next Steps:")
    print("   - Integrate these components into the main app.py")
    print("   - Add websocket support for true real-time streaming")
    print("   - Implement query result caching with similarity matching")
    print("   - Add support for multiple LLM providers")


if __name__ == "__main__":
    main()